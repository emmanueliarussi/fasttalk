{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c695c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/mnt/Datasets/expressive_ft/npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "npz_files = sorted(glob.glob(os.path.join(data_path, \"*.npz\")))\n",
    "if not npz_files:\n",
    "    print(f\"No .npz files found in {data_path}\")\n",
    "else:\n",
    "    for npz_path in npz_files:\n",
    "        with np.load(npz_path) as data:\n",
    "            print(f\"{os.path.basename(npz_path)} keys: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = np.load(npz_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor['jaw'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/mnt/fasttalk\")\n",
    "import glob\n",
    "import shutil\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import librosa\n",
    "from types import SimpleNamespace\n",
    "from base.baseTrainer import load_state_dict\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "from renderer.renderer import Renderer\n",
    "from flame_model.FLAME import FLAMEModel\n",
    "from pytorch3d.transforms import rotation_6d_to_matrix, matrix_to_euler_angles\n",
    "from models import get_model\n",
    "\n",
    "audio_dir = \"/mnt/Datasets/expressive_ft/wav\"\n",
    "real_video_dir = \"/mnt/Datasets/expressive_ft/synthetic_dataset\"\n",
    "output_dir = \"/mnt/fasttalk/demo/video_expressive_ft\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "renderer = Renderer(render_full_head=True).to(device)\n",
    "flame = FLAMEModel(n_shape=300, n_exp=50).to(device)\n",
    "\n",
    "POSE_IS_6D = True\n",
    "CHUNK_SIZE = 50  # reduce if still OOM\n",
    "FPS = 24\n",
    "EPS = 1e-8\n",
    "\n",
    "# ffmpeg speed knobs\n",
    "FFMPEG_PRESET = \"fast\"\n",
    "FFMPEG_CRF = \"18\"\n",
    "\n",
    "def load_and_flatten_yaml(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        full_config = yaml.safe_load(f)\n",
    "    flattened_config = {}\n",
    "    for top_level_key, sub_dict in full_config.items():\n",
    "        if isinstance(sub_dict, dict):\n",
    "            for k, v in sub_dict.items():\n",
    "                flattened_config[k] = v\n",
    "        else:\n",
    "            flattened_config[top_level_key] = sub_dict\n",
    "    return SimpleNamespace(**flattened_config)\n",
    "\n",
    "def load_model_for_eval(checkpoint_path, cfg):\n",
    "    model = get_model(cfg)\n",
    "    model = model.to(device)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    if \"state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model_for_eval_state_dict(checkpoint_path, cfg):\n",
    "    model = get_model(cfg)\n",
    "    model = model.to(device)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage.cpu())\n",
    "    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n",
    "        load_state_dict(model, checkpoint[\"state_dict\"], strict=False)\n",
    "    else:\n",
    "        load_state_dict(model, checkpoint, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "cfg_s1 = load_and_flatten_yaml(\"/mnt/fasttalk/config/talkinghead-1kh/stage1.yaml\")\n",
    "cfg_s1.batch_size = 1\n",
    "checkpoint_path_s1 = \"/mnt/fasttalk/logs/talkinghead/talkinghead-s1/model_200/model.pth.tar\"\n",
    "vq_model = load_model_for_eval(checkpoint_path_s1, cfg_s1)\n",
    "\n",
    "cfg_s2 = load_and_flatten_yaml(\"/mnt/fasttalk/config/talkinghead-1kh/stage2finetunning.yaml\")\n",
    "cfg_s2.batch_size = 1\n",
    "base_checkpoint_path_s2 = \"/mnt/fasttalk/logs/talkinghead/talkinghead-s2/model_260/model.pth.tar\"\n",
    "finetuned_checkpoint_path_s2 = \"/mnt/fasttalk/logs/talkinghead/talkinghead-s2-finetunning/model_20/model.pth.tar\"\n",
    "\n",
    "# Load pretrained S2 first, initialize residual base, then load finetuned weights\n",
    "s2_model = get_model(cfg_s2).to(device)\n",
    "checkpoint_base = torch.load(base_checkpoint_path_s2, map_location=lambda storage, loc: storage.cpu())\n",
    "if isinstance(checkpoint_base, dict) and \"state_dict\" in checkpoint_base:\n",
    "    load_state_dict(s2_model, checkpoint_base[\"state_dict\"], strict=False)\n",
    "else:\n",
    "    load_state_dict(s2_model, checkpoint_base, strict=False)\n",
    "s2_model.eval()\n",
    "if getattr(cfg_s2, \"use_residual\", False):\n",
    "    s2_model.init_residual_base()\n",
    "\n",
    "checkpoint_ft = torch.load(finetuned_checkpoint_path_s2, map_location=lambda storage, loc: storage.cpu())\n",
    "if isinstance(checkpoint_ft, dict) and \"state_dict\" in checkpoint_ft:\n",
    "    load_state_dict(s2_model, checkpoint_ft[\"state_dict\"], strict=False)\n",
    "else:\n",
    "    load_state_dict(s2_model, checkpoint_ft, strict=False)\n",
    "s2_model.eval()\n",
    "\n",
    "wav2vec_processor = Wav2Vec2FeatureExtractor.from_pretrained(cfg_s2.wav2vec2model_path)\n",
    "\n",
    "def to_euler_xyz(pose_tensor):\n",
    "    if pose_tensor.shape[-1] == 3:\n",
    "        return pose_tensor\n",
    "    if pose_tensor.shape[-1] == 6:\n",
    "        if not POSE_IS_6D:\n",
    "            return pose_tensor[:, :3]\n",
    "        rot_mats = rotation_6d_to_matrix(pose_tensor)\n",
    "        return matrix_to_euler_angles(rot_mats, \"XYZ\")\n",
    "    if pose_tensor.shape[-1] == 9:\n",
    "        rot_mats = pose_tensor.view(-1, 3, 3)\n",
    "        return matrix_to_euler_angles(rot_mats, \"XYZ\")\n",
    "    raise ValueError(f\"Unsupported pose shape: {pose_tensor.shape}\")\n",
    "\n",
    "def get_vertices_from_blendshapes(expr_tensor, gpose_tensor, jaw_tensor, eyelids_tensor, shape_tensor):\n",
    "    # Same mechanism as test_talkinghead-1kh_vq_bs.ipynb, now with shape\n",
    "    target_shape_tensor = shape_tensor\n",
    "    eye = matrix_to_euler_angles(torch.eye(3)[None].to(expr_tensor.device), \"XYZ\").squeeze(0)\n",
    "    eyes = torch.cat([eye, eye], dim=0).unsqueeze(0).expand(expr_tensor.shape[0], -1)\n",
    "    pose = torch.cat([gpose_tensor, jaw_tensor], dim=-1)\n",
    "    verts, _ = flame.forward(\n",
    "        shape_params=target_shape_tensor,\n",
    "        expression_params=expr_tensor,\n",
    "        pose_params=pose,\n",
    "        eye_pose_params=eyes\n",
    "    )\n",
    "    return verts.detach()\n",
    "\n",
    "def extract_blendshapes_from_npz(npz_data):\n",
    "    # exp\n",
    "    if \"exp\" in npz_data.files:\n",
    "        exp = npz_data[\"exp\"]\n",
    "    elif \"expression_params\" in npz_data.files:\n",
    "        exp = npz_data[\"expression_params\"]\n",
    "    else:\n",
    "        return None\n",
    "    # shape\n",
    "    shape = None\n",
    "    if \"shape\" in npz_data.files:\n",
    "        shape = npz_data[\"shape\"]\n",
    "    elif \"shape_params\" in npz_data.files:\n",
    "        shape = npz_data[\"shape_params\"]\n",
    "    elif \"beta\" in npz_data.files:\n",
    "        shape = npz_data[\"beta\"]\n",
    "    # pose / gpose / jaw\n",
    "    pose = None\n",
    "    if \"pose\" in npz_data.files:\n",
    "        pose = npz_data[\"pose\"]\n",
    "    elif \"pose_params\" in npz_data.files:\n",
    "        pose = npz_data[\"pose_params\"]\n",
    "    elif \"gpose\" in npz_data.files:\n",
    "        pose = npz_data[\"gpose\"]\n",
    "    jaw = None\n",
    "    if \"jaw\" in npz_data.files:\n",
    "        jaw = npz_data[\"jaw\"]\n",
    "    elif \"jaw_params\" in npz_data.files:\n",
    "        jaw = npz_data[\"jaw_params\"]\n",
    "\n",
    "    exp_t = torch.from_numpy(exp.reshape(-1, 50)).float().to(device)\n",
    "    T = exp_t.shape[0]\n",
    "    if shape is None:\n",
    "        shape_t = torch.zeros((T, 300), device=device)\n",
    "    else:\n",
    "        shape_arr = shape.reshape(-1, shape.shape[-1])\n",
    "        shape_t = torch.from_numpy(shape_arr).float().to(device)\n",
    "        if shape_t.shape[0] == 1 and T > 1:\n",
    "            shape_t = shape_t.expand(T, -1)\n",
    "        elif shape_t.shape[0] != T:\n",
    "            shape_t = shape_t[:T] if shape_t.shape[0] > T else shape_t.expand(T, -1)\n",
    "        if shape_t.shape[-1] < 300:\n",
    "            pad = torch.zeros((shape_t.shape[0], 300 - shape_t.shape[-1]), device=device)\n",
    "            shape_t = torch.cat([shape_t, pad], dim=-1)\n",
    "        elif shape_t.shape[-1] > 300:\n",
    "            shape_t = shape_t[:, :300]\n",
    "    if pose is not None:\n",
    "        pose_t = torch.from_numpy(pose.reshape(-1, pose.shape[-1])).float().to(device)\n",
    "    else:\n",
    "        pose_t = torch.zeros((T, 3), device=device)\n",
    "\n",
    "    # Always treat pose as global pose (convert 6D -> 3D Euler)\n",
    "    gpose_t = to_euler_xyz(pose_t)\n",
    "    if jaw is None:\n",
    "        jaw_t = torch.zeros((gpose_t.shape[0], 3), device=device)\n",
    "    else:\n",
    "        jaw_t = torch.from_numpy(jaw.reshape(-1, jaw.shape[-1])).float().to(device)\n",
    "        jaw_t = to_euler_xyz(jaw_t)\n",
    "\n",
    "    # eyelids\n",
    "    if \"eyelids\" in npz_data.files:\n",
    "        eyelids = npz_data[\"eyelids\"]\n",
    "        eyelids_t = torch.from_numpy(eyelids.reshape(-1, eyelids.shape[-1])).float().to(device)\n",
    "    else:\n",
    "        eyelids_t = torch.ones((gpose_t.shape[0], 2), device=device)\n",
    "\n",
    "    blendshapes = torch.cat([exp_t, gpose_t, jaw_t, eyelids_t], dim=-1)\n",
    "    return blendshapes, shape_t\n",
    "\n",
    "def load_style_tensor(style_path):\n",
    "    try:\n",
    "        loaded = torch.load(style_path, map_location=device)\n",
    "        if torch.is_tensor(loaded):\n",
    "            style_t = loaded.float().to(device)\n",
    "        elif isinstance(loaded, np.ndarray):\n",
    "            style_t = torch.from_numpy(loaded).float().to(device)\n",
    "        else:\n",
    "            style_t = None\n",
    "    except Exception:\n",
    "        style_t = None\n",
    "    if style_t is None:\n",
    "        npz_data = np.load(style_path, allow_pickle=True)\n",
    "        if isinstance(npz_data, np.lib.npyio.NpzFile):\n",
    "            extracted = extract_blendshapes_from_npz(npz_data)\n",
    "            if extracted is None:\n",
    "                key = npz_data.files[0]\n",
    "                style_t = torch.from_numpy(npz_data[key]).float().to(device)\n",
    "            else:\n",
    "                style_t = extracted[0]\n",
    "        else:\n",
    "            style_t = torch.from_numpy(npz_data).float().to(device)\n",
    "    if style_t.dim() == 2:\n",
    "        style_t = style_t.unsqueeze(0)\n",
    "    return style_t\n",
    "\n",
    "def find_audio_for_stem(stem):\n",
    "    for ext in (\".wav\", \".mp3\", \".flac\", \".m4a\", \".aac\", \".ogg\"):\n",
    "        candidate = os.path.join(audio_dir, stem + ext)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "    matches = sorted(glob.glob(os.path.join(audio_dir, stem + \".*\")))\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "def find_real_video_for_stem(stem):\n",
    "    # direct match in root\n",
    "    for ext in (\".mp4\", \".mov\", \".mkv\", \".avi\", \".webm\"):\n",
    "        candidate = os.path.join(real_video_dir, stem + ext)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "    # recursive search\n",
    "    pattern = os.path.join(real_video_dir, \"**\", stem + \".*\")\n",
    "    matches = sorted(glob.glob(pattern, recursive=True))\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "def render_sequence_from_blendshapes(blendshapes, shape_t, out_video_path, fps=FPS, chunk_size=CHUNK_SIZE):\n",
    "    try:\n",
    "        import imageio.v2 as imageio\n",
    "    except Exception as exc:\n",
    "        print(f\"imageio not available: {exc}\")\n",
    "        return False\n",
    "    T = blendshapes.shape[0]\n",
    "    cam_base = torch.tensor([5, 0, 0], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    # Prefer ffmpeg CLI (matches test_talkinghead behavior)\n",
    "    if shutil.which(\"ffmpeg\"):\n",
    "        seq_dir = out_video_path + \"_frames\"\n",
    "        os.makedirs(seq_dir, exist_ok=True)\n",
    "        for start in range(0, T, chunk_size):\n",
    "            end = min(T, start + chunk_size)\n",
    "            chunk = blendshapes[start:end]\n",
    "            expr = chunk[:, :50]\n",
    "            gpose = chunk[:, 50:53]\n",
    "            jaw = chunk[:, 53:56]\n",
    "            eyelids = chunk[:, 56:]\n",
    "            shape_chunk = shape_t[start:end]\n",
    "            with torch.no_grad():\n",
    "                verts = get_vertices_from_blendshapes(expr, gpose, jaw, eyelids, shape_chunk)\n",
    "                cam = cam_base.expand(verts.shape[0], -1)\n",
    "                frames = renderer.forward(verts, cam)[\"rendered_img\"]\n",
    "            frames_np = (frames.detach().cpu().numpy().transpose(0, 2, 3, 1) * 255).clip(0, 255).astype(np.uint8)\n",
    "            for i, frame in enumerate(frames_np, start=start):\n",
    "                imageio.imwrite(os.path.join(seq_dir, f\"{i:06d}.png\"), frame)\n",
    "            del verts, frames, frames_np\n",
    "            torch.cuda.empty_cache()\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\",\n",
    "            \"-framerate\", str(fps),\n",
    "            \"-i\", os.path.join(seq_dir, \"%06d.png\"),\n",
    "            \"-c:v\", \"libx264\",\n",
    "            \"-preset\", FFMPEG_PRESET,\n",
    "            \"-crf\", FFMPEG_CRF,\n",
    "            \"-pix_fmt\", \"yuv420p\",\n",
    "            out_video_path\n",
    "        ]\n",
    "        try:\n",
    "            subprocess.run(cmd, check=True)\n",
    "            return True\n",
    "        except Exception as exc:\n",
    "            print(f\"ffmpeg encode failed: {exc}\")\n",
    "            return False\n",
    "    # Fallback to imageio FFMPEG writer if available\n",
    "    try:\n",
    "        writer = imageio.get_writer(out_video_path, fps=fps, codec=\"libx264\", format=\"FFMPEG\")\n",
    "    except Exception as exc:\n",
    "        print(f\"FFMPEG writer not available ({exc}); falling back to PNG sequence.\")\n",
    "        seq_dir = out_video_path + \"_frames\"\n",
    "        os.makedirs(seq_dir, exist_ok=True)\n",
    "        for start in range(0, T, chunk_size):\n",
    "            end = min(T, start + chunk_size)\n",
    "            chunk = blendshapes[start:end]\n",
    "            expr = chunk[:, :50]\n",
    "            gpose = chunk[:, 50:53]\n",
    "            jaw = chunk[:, 53:56]\n",
    "            eyelids = chunk[:, 56:]\n",
    "            shape_chunk = shape_t[start:end]\n",
    "            with torch.no_grad():\n",
    "                verts = get_vertices_from_blendshapes(expr, gpose, jaw, eyelids, shape_chunk)\n",
    "                cam = cam_base.expand(verts.shape[0], -1)\n",
    "                frames = renderer.forward(verts, cam)[\"rendered_img\"]\n",
    "            frames_np = (frames.detach().cpu().numpy().transpose(0, 2, 3, 1) * 255).clip(0, 255).astype(np.uint8)\n",
    "            for i, frame in enumerate(frames_np, start=start):\n",
    "                imageio.imwrite(os.path.join(seq_dir, f\"{i:06d}.png\"), frame)\n",
    "            del verts, frames, frames_np\n",
    "            torch.cuda.empty_cache()\n",
    "        return True\n",
    "    with writer:\n",
    "        for start in range(0, T, chunk_size):\n",
    "            end = min(T, start + chunk_size)\n",
    "            chunk = blendshapes[start:end]\n",
    "            expr = chunk[:, :50]\n",
    "            gpose = chunk[:, 50:53]\n",
    "            jaw = chunk[:, 53:56]\n",
    "            eyelids = chunk[:, 56:]\n",
    "            shape_chunk = shape_t[start:end]\n",
    "            with torch.no_grad():\n",
    "                verts = get_vertices_from_blendshapes(expr, gpose, jaw, eyelids, shape_chunk)\n",
    "                cam = cam_base.expand(verts.shape[0], -1)\n",
    "                frames = renderer.forward(verts, cam)[\"rendered_img\"]\n",
    "            frames_np = (frames.detach().cpu().numpy().transpose(0, 2, 3, 1) * 255).clip(0, 255).astype(np.uint8)\n",
    "            for frame in frames_np:\n",
    "                writer.append_data(frame)\n",
    "            del verts, frames, frames_np\n",
    "            torch.cuda.empty_cache()\n",
    "    return True\n",
    "\n",
    "def decode_with_vq_model(blendshapes, model):\n",
    "    blendshapes_b = blendshapes.unsqueeze(0)\n",
    "    mask = torch.ones((1, blendshapes_b.shape[1]), device=blendshapes_b.device, dtype=torch.bool)\n",
    "    with torch.no_grad():\n",
    "        decoded, _ = model(blendshapes_b, mask)\n",
    "    return decoded.squeeze(0)\n",
    "\n",
    "def predict_from_audio(audio_path, model, target_style):\n",
    "    speech_array, _ = librosa.load(audio_path, sr=16000)\n",
    "    audio_feature = np.squeeze(wav2vec_processor(speech_array, sampling_rate=16000).input_values)\n",
    "    audio_feature = np.reshape(audio_feature, (-1, audio_feature.shape[0]))\n",
    "    audio_feature = torch.FloatTensor(audio_feature).to(device=device)\n",
    "    with torch.no_grad():\n",
    "        blendshapes_out = model.predict_no_quantizer(audio_feature, target_style=target_style)\n",
    "    return blendshapes_out.squeeze(0)\n",
    "\n",
    "style_path = \"/mnt/fasttalk/demo/styles/style_2.npz\"\n",
    "target_style = None\n",
    "\n",
    "for npz_path in npz_files:\n",
    "    stem = os.path.splitext(os.path.basename(npz_path))[0]\n",
    "    with np.load(npz_path) as npz_data:\n",
    "        extracted = extract_blendshapes_from_npz(npz_data)\n",
    "        if extracted is None:\n",
    "            print(f\"Skipping {npz_path}: missing exp/pose data.\")\n",
    "            continue\n",
    "        blendshapes, shape_t = extracted\n",
    "        duration_sec = float(blendshapes.shape[0]) / float(FPS)\n",
    "        out_video = os.path.join(output_dir, f\"{stem}.mp4\")\n",
    "        print(f\"Rendering {stem} -> {out_video}\")\n",
    "        ok = render_sequence_from_blendshapes(blendshapes, shape_t, out_video, fps=FPS, chunk_size=CHUNK_SIZE)\n",
    "        if not ok:\n",
    "            continue\n",
    "        # FASTTALK S1: encode/decode and render\n",
    "        s1_blendshapes = decode_with_vq_model(blendshapes, vq_model)\n",
    "        s1_video = os.path.join(output_dir, f\"{stem}_s1.mp4\")\n",
    "        ok_s1 = render_sequence_from_blendshapes(s1_blendshapes, shape_t, s1_video, fps=FPS, chunk_size=CHUNK_SIZE)\n",
    "        if not ok_s1:\n",
    "            print(f\"S1 render failed for {stem}\")\n",
    "            continue\n",
    "        # Audio-only (S2) prediction with fixed style\n",
    "        audio_path = find_audio_for_stem(stem)\n",
    "        s2_video = None\n",
    "        if audio_path:\n",
    "            s2_blendshapes = predict_from_audio(audio_path, s2_model, target_style)\n",
    "            min_len = min(s2_blendshapes.shape[0], shape_t.shape[0])\n",
    "            s2_blendshapes = s2_blendshapes[:min_len]\n",
    "            shape_t_s2 = shape_t[:min_len]\n",
    "            s2_video = os.path.join(output_dir, f\"{stem}_s2_audio.mp4\")\n",
    "            ok_s2 = render_sequence_from_blendshapes(s2_blendshapes, shape_t_s2, s2_video, fps=FPS, chunk_size=CHUNK_SIZE)\n",
    "            if not ok_s2:\n",
    "                print(f\"S2 audio-only render failed for {stem}\")\n",
    "                s2_video = None\n",
    "        else:\n",
    "            print(f\"No audio found for S2 prediction: {stem}\")\n",
    "        # Side-by-side with real video (real left, tracking right), using real audio\n",
    "        real_video = find_real_video_for_stem(stem)\n",
    "        if shutil.which(\"ffmpeg\"):\n",
    "            if real_video:\n",
    "                print(f\"Real video found: {real_video}\")\n",
    "                side_by_side = os.path.join(output_dir, f\"{stem}_side_by_side.mp4\")\n",
    "                cmd = [\n",
    "                    \"ffmpeg\", \"-y\",\n",
    "                    \"-i\", real_video,\n",
    "                    \"-i\", out_video,\n",
    "                    \"-filter_complex\",\n",
    "                    \"[0:v]setpts=PTS-STARTPTS[v0];[1:v]setpts=PTS-STARTPTS[v1];[v0][v1]hstack=inputs=2[v]\",\n",
    "                    \"-map\", \"[v]\",\n",
    "                    \"-map\", \"0:a?\",\n",
    "                    \"-t\", str(duration_sec),\n",
    "                    \"-c:v\", \"libx264\",\n",
    "                    \"-preset\", FFMPEG_PRESET,\n",
    "                    \"-crf\", FFMPEG_CRF,\n",
    "                    \"-c:a\", \"copy\",\n",
    "                    \"-shortest\",\n",
    "                    side_by_side\n",
    "                ]\n",
    "                try:\n",
    "                    subprocess.run(cmd, check=True)\n",
    "                    print(f\"Saved side-by-side: {side_by_side}\")\n",
    "                except Exception as exc:\n",
    "                    print(f\"Side-by-side failed for {stem}: {exc}\")\n",
    "            else:\n",
    "                print(f\"No real video found for {stem}\")\n",
    "        if shutil.which(\"ffmpeg\"):\n",
    "            audio_source = None\n",
    "            if audio_path:\n",
    "                audio_source = audio_path\n",
    "                print(f\"Audio file found: {audio_source}\")\n",
    "            elif real_video:\n",
    "                audio_source = real_video\n",
    "                print(f\"Using audio from real video: {audio_source}\")\n",
    "            if audio_source:\n",
    "                out_with_audio = os.path.join(output_dir, f\"{stem}_audio.mp4\")\n",
    "                cmd = [\n",
    "                    \"ffmpeg\", \"-y\",\n",
    "                    \"-i\", out_video,\n",
    "                    \"-i\", audio_source,\n",
    "                    \"-map\", \"0:v\",\n",
    "                    \"-map\", \"1:a?\",\n",
    "                    \"-t\", str(duration_sec),\n",
    "                    \"-c:v\", \"copy\",\n",
    "                    \"-c:a\", \"aac\",\n",
    "                    \"-shortest\",\n",
    "                    out_with_audio\n",
    "                ]\n",
    "                try:\n",
    "                    subprocess.run(cmd, check=True)\n",
    "                    print(f\"Saved with audio: {out_with_audio}\")\n",
    "                except Exception as exc:\n",
    "                    print(f\"Audio mux failed for {stem}: {exc}\")\n",
    "            else:\n",
    "                print(f\"No matching audio or real video found for {stem}\")\n",
    "        elif audio_path or real_video:\n",
    "            print(f\"ffmpeg not found; audio not muxed for {stem}\")\n",
    "        else:\n",
    "            print(f\"No matching audio or real video found for {stem}\")\n",
    "        # Quad concat: GT / SMIRK TRACKING / FASTTALK S1 / AUDIO-ONLY (S2)\n",
    "        if shutil.which(\"ffmpeg\") and real_video and s1_video and s2_video:\n",
    "            quad_out = os.path.join(output_dir, f\"{stem}_gt_tracking_s1_s2audio.mp4\")\n",
    "            cmd = [\n",
    "                \"ffmpeg\", \"-y\",\n",
    "                \"-i\", real_video,\n",
    "                \"-i\", out_video,\n",
    "                \"-i\", s1_video,\n",
    "                \"-i\", s2_video,\n",
    "                \"-filter_complex\",\n",
    "                \"[0:v]setpts=PTS-STARTPTS[v0];[1:v]setpts=PTS-STARTPTS[v1];[2:v]setpts=PTS-STARTPTS[v2];[3:v]setpts=PTS-STARTPTS[v3];[v0][v1][v2][v3]hstack=inputs=4[v]\",\n",
    "                \"-map\", \"[v]\",\n",
    "                \"-map\", \"0:a?\",\n",
    "                \"-t\", str(duration_sec),\n",
    "                \"-c:v\", \"libx264\",\n",
    "                \"-preset\", FFMPEG_PRESET,\n",
    "                \"-crf\", FFMPEG_CRF,\n",
    "                \"-c:a\", \"copy\",\n",
    "                \"-shortest\",\n",
    "                quad_out\n",
    "            ]\n",
    "            try:\n",
    "                subprocess.run(cmd, check=True)\n",
    "                print(f\"Saved quad concat: {quad_out}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"Quad concat failed for {stem}: {exc}\")\n",
    "        elif shutil.which(\"ffmpeg\"):\n",
    "            print(f\"Quad concat skipped for {stem} (missing real/s1/s2 video)\")\n",
    "        else:\n",
    "            print(f\"ffmpeg not found; quad concat not created for {stem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a761a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6ff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasttalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
