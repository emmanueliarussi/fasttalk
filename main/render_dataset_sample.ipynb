{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# change base folder\n",
    "os.chdir('../')\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from flame.flame import FlameHead\n",
    "from renderer.renderer import Renderer\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.transforms import matrix_to_euler_angles\n",
    "import subprocess\n",
    "import tempfile \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n",
    "import glob\n",
    "from models.stage2 import CodeTalker\n",
    "import yaml\n",
    "from models import get_model\n",
    "from base.baseTrainer import load_state_dict\n",
    "from types import SimpleNamespace\n",
    "from transformers import AutoProcessor, Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flame    = FlameHead(shape_params=300,expr_params=50).to(device)\n",
    "renderer = Renderer(render_full_head=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertices_from_blendshapes(expr, jaw, neck=None):\n",
    "    # Load the encoded file\n",
    "    expr_tensor =  expr.to(device)\n",
    "    jaw_tensor  =  jaw.to(device) #torch.zeros(expr_tensor.shape[0],3).to(device)\n",
    "\n",
    "    target_shape_tensor = torch.zeros(expr_tensor.shape[0], 300).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    I = matrix_to_euler_angles(torch.cat([torch.eye(3)[None]], dim=0),\"XYZ\").to(device)\n",
    "\n",
    "    eye_r    = I.clone().to(device).squeeze()\n",
    "    eye_l    = I.clone().to(device).squeeze()\n",
    "    eyes     = torch.cat([eye_r,eye_l],dim=0).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    translation = torch.zeros(expr_tensor.shape[0], 3).to(device)\n",
    "\n",
    "    if neck==None:\n",
    "        neck = I.clone().expand(expr_tensor.shape[0], -1).to(device)\n",
    "    \n",
    "    rotation = I.clone().expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    # Compute Flame\n",
    "    flame_output_only_shape   = flame.forward(target_shape_tensor, expr_tensor, rotation, neck, jaw_tensor, eyes, translation, return_landmarks=False)\n",
    "\n",
    "    return flame_output_only_shape.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(frame_inx, renderer_output_blendshapes, axes):\n",
    "    # Select the frames to plot\n",
    "    frame = renderer_output_blendshapes['rendered_img'][frame_inx].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Update the second subplot\n",
    "    axes.clear()\n",
    "    axes.imshow((frame * 255).astype(np.uint8))\n",
    "    axes.axis('off')\n",
    "    axes.set_title(f'Frame Stage 1 (Blendshape) {frame_inx + 1}')\n",
    "\n",
    "def lowpass_filter(tensor, kernel_size=10):\n",
    "    # tensor: [seq_len, 3]\n",
    "    tensor = tensor.unsqueeze(0).transpose(1,2)  # -> [1, 3, seq_len]\n",
    "    \n",
    "    # Create a 1D uniform kernel\n",
    "    kernel = torch.ones(1, 1, kernel_size, device=tensor.device) / kernel_size\n",
    "    \n",
    "    # Apply same kernel to each channel (grouped conv)\n",
    "    filtered = F.conv1d(tensor, kernel.expand(3, -1, -1), padding=kernel_size//2, groups=3)\n",
    "    \n",
    "    return filtered.transpose(1,2).squeeze(0)  # -> back to [seq_len, 3]\n",
    "\n",
    "# Function to create and save the video\n",
    "def create_and_save_video(encoded_dir,file_name, renderer,audio_dir,output_dir):\n",
    "    base_name = os.path.basename(file_name).replace('.npz', '')\n",
    "    print(base_name)\n",
    "    \n",
    "    blendshapes_data_encoded_expr = np.load(f'{encoded_dir}/{base_name}.npz')['exp'].reshape(-1, 50)\n",
    "    blendshapes_data_encoded_jaw  = np.load(f'{encoded_dir}/{base_name}.npz')['pose'][:,3:6].reshape(-1, 3)\n",
    "    blendshapes_data_encoded_neck  = np.load(f'{encoded_dir}/{base_name}.npz')[\"pose\"][:,0:3].reshape(-1, 3)\n",
    "\n",
    "    blendshapes_data_encoded_expr = torch.tensor(blendshapes_data_encoded_expr, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_jaw  = torch.tensor(blendshapes_data_encoded_jaw, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_neck = torch.tensor(blendshapes_data_encoded_neck, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Flip neck horizontally to avoid bias\n",
    "    blendshapes_data_encoded_neck_flipped = blendshapes_data_encoded_neck.clone() \n",
    "    blendshapes_data_encoded_neck_flipped[:, [1, 2]] = blendshapes_data_encoded_neck[:, [2, 1]]\n",
    "    blendshapes_data_encoded_neck_flipped = lowpass_filter(blendshapes_data_encoded_neck_flipped, kernel_size=9)\n",
    "\n",
    "    # Compute mean across seq (dim 0), keeping the 3 features\n",
    "    mean_per_component = blendshapes_data_encoded_neck_flipped.mean(dim=0)  # [3]\n",
    "\n",
    "    # Subtract mean from every element\n",
    "    blendshapes_data_encoded_neck_flipped = blendshapes_data_encoded_neck_flipped - mean_per_component\n",
    "\n",
    "    print(blendshapes_data_encoded_expr.shape)\n",
    "    print(blendshapes_data_encoded_jaw.shape)   \n",
    "    print(blendshapes_data_encoded_neck_flipped.shape)\n",
    "    print(\"----\")\n",
    "\n",
    "    # Compute vertices from blendshapes\n",
    "    blendshapes_derived_vertices = get_vertices_from_blendshapes(blendshapes_data_encoded_expr,blendshapes_data_encoded_jaw, blendshapes_data_encoded_neck_flipped)\n",
    "    print(blendshapes_derived_vertices.shape)\n",
    "    \n",
    "    # Fixed camera\n",
    "    cam_original = torch.tensor([10,0,0], dtype=torch.float32).expand(blendshapes_derived_vertices.shape[0], -1).to(device)\n",
    "    print(cam_original.shape)\n",
    "\n",
    "    # Render the frames\n",
    "    renderer_output_blendshapes  = renderer.forward(blendshapes_derived_vertices, cam_original)\n",
    "\n",
    "    N = renderer_output_blendshapes['rendered_img'].shape[0] # Number of frames\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "    # Create an animation\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, \n",
    "        update, \n",
    "        frames=N, \n",
    "        fargs=(renderer_output_blendshapes, axes),\n",
    "        interval=100\n",
    "    )\n",
    "\n",
    "    # Save the animation as a video file\n",
    "    video_file = f'{output_dir}/{base_name}.mp4'\n",
    "    ani.save(video_file, writer='ffmpeg', fps=25)\n",
    "    print(f\"Video saved as {video_file}\")\n",
    "    \n",
    "    # =============== Add audio to the video ===============\n",
    "    \n",
    "    # Add audio to the video\n",
    "    audio_file = f'{audio_dir}/{base_name}.wav'\n",
    "    output_with_audio = f'{output_dir}/{base_name}_with_audio.mp4'\n",
    "    if os.path.exists(audio_file):\n",
    "        cmd = f'ffmpeg -y -i {video_file} -i {audio_file} -c:v copy -c:a aac -strict experimental {output_with_audio}'\n",
    "        subprocess.run(cmd, shell=True)\n",
    "        print(f\"Video with audio saved as {output_with_audio}\")\n",
    "    else:\n",
    "        print(f\"Audio file {audio_file} not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing encoded files\n",
    "encoded_dir = '/root/Datasets/ensemble_dataset/npz'\n",
    "audio_dir   = '/root/Datasets/ensemble_dataset/wav'\n",
    "output_dir  = 'demo/video'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Directory created: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {output_dir}\")\n",
    "\n",
    "counter = 1\n",
    "# Iterate over all files in the encoded directory\n",
    "for file_name in os.listdir(encoded_dir):\n",
    "    if counter == 0:\n",
    "        break\n",
    "    if file_name.endswith('.npz'):\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        create_and_save_video(encoded_dir,file_name,renderer,audio_dir,output_dir)\n",
    "    counter -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75b045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasttalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
