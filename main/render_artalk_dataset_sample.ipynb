{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c36273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# change base folder\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from flame_model.FLAME import FLAMEModel\n",
    "from renderer.renderer import Renderer\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.transforms import matrix_to_euler_angles\n",
    "import subprocess\n",
    "import tempfile \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n",
    "import glob\n",
    "from models.stage2 import CodeTalker\n",
    "import yaml\n",
    "from models import get_model\n",
    "from base.baseTrainer import load_state_dict\n",
    "from types import SimpleNamespace\n",
    "from transformers import AutoProcessor, Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "import pickle\n",
    "import itertools\n",
    "from pytorch3d.renderer import look_at_view_transform\n",
    "import random\n",
    "from scipy.signal import savgol_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flame    = FLAMEModel(n_shape=300,n_exp=50).to(device)\n",
    "renderer = Renderer(render_full_head=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertices_from_blendshapes(expr, gpose, jaw, eyelids=None):\n",
    "\n",
    "    # Load the encoded file\n",
    "    expr_tensor    = expr.to(device)\n",
    "    gpose_tensor   = gpose.to(device)\n",
    "    jaw_tensor     = jaw.to(device)\n",
    "    \n",
    "    if eyelids is not None:\n",
    "        eyelids_tensor = eyelids.to(device)\n",
    "\n",
    "    target_shape_tensor = torch.zeros(expr_tensor.shape[0], 300).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    I = matrix_to_euler_angles(torch.cat([torch.eye(3)[None]], dim=0),\"XYZ\").to(device)\n",
    "\n",
    "    eye_r    = I.clone().to(device).squeeze()\n",
    "    eye_l    = I.clone().to(device).squeeze()\n",
    "    eyes     = torch.cat([eye_r,eye_l],dim=0).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    pose = torch.cat([gpose_tensor, jaw_tensor], dim=-1).to(device)\n",
    "\n",
    "    flame_output_only_shape,_ = flame.forward(shape_params=target_shape_tensor, \n",
    "                                               expression_params=expr_tensor, \n",
    "                                               pose_params=pose, \n",
    "                                               eye_pose_params=eyes)\n",
    "    return flame_output_only_shape.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(frame_inx, renderer_output_blendshapes, axes):\n",
    "    # Select the frames to plot\n",
    "    frame = renderer_output_blendshapes[frame_inx].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Update the second subplot\n",
    "    axes.clear()\n",
    "    axes.imshow((frame * 255).astype(np.uint8))\n",
    "    axes.set_position([0, 0, 1, 1])\n",
    "    axes.axis('off')\n",
    "    #axes.set_title(f'Frame Stage 1 (Blendshape) {frame_inx + 1}')\n",
    "\n",
    "# Function to create and save the video\n",
    "def create_and_save_video(encoded_dir,file_name, renderer,audio_dir,output_dir):\n",
    "    base_name = os.path.basename(file_name).replace('.npz', '')\n",
    "    print(base_name)\n",
    "    \n",
    "    flame_param = np.load(f'{encoded_dir}/{base_name}.npz')\n",
    "\n",
    "    for key in flame_param.keys():\n",
    "        print(key, flame_param[key].shape)\n",
    "\n",
    "    if 'pose' in flame_param:\n",
    "        blendshapes_data_encoded_expr   = flame_param['exp'].reshape(-1, 50)\n",
    "        blendshapes_data_encoded_jaw    = flame_param[\"pose\"][:,3:6].reshape(-1, 3)\n",
    "        blendshapes_data_encoded_gpose  = flame_param[\"pose\"][:,0:3].reshape(-1, 3)\n",
    "        blendshapes_data_encoded_gpose  = blendshapes_data_encoded_gpose - blendshapes_data_encoded_gpose.mean(axis=0, keepdims=True)\n",
    "    elif 'pose_params' in flame_param:\n",
    "        blendshapes_data_encoded_expr   = flame_param['expression_params'].reshape(-1, 50)\n",
    "        blendshapes_data_encoded_jaw    = flame_param[\"jaw_params\"].reshape(-1, 3)\n",
    "        blendshapes_data_encoded_gpose  = flame_param[\"pose_params\"].reshape(-1, 3)\n",
    "        blendshapes_data_encoded_gpose  = blendshapes_data_encoded_gpose - blendshapes_data_encoded_gpose.mean(axis=0, keepdims=True)\n",
    "    else:\n",
    "        blendshapes_data_encoded_expr    = flame_param['exp'].reshape(-1, 50)\n",
    "        blendshapes_data_encoded_gpose   = flame_param[\"gpose\"].reshape(-1, 3)\n",
    "        blendshapes_data_encoded_jaw     = flame_param['jaw'].reshape(-1, 3)\n",
    "        #blendshapes_data_encoded_eyelids = flame_param['eyelids'].reshape(-1, 2)\n",
    "    \n",
    "    blendshapes_data_encoded_gpose = savgol_filter(blendshapes_data_encoded_gpose, window_length=7, polyorder=2, axis=0)\n",
    "\n",
    "    print(\"expr \",blendshapes_data_encoded_expr.shape)\n",
    "    print(\"gpose \",blendshapes_data_encoded_gpose.shape)\n",
    "    print(\"jaw \", blendshapes_data_encoded_jaw.shape)\n",
    "    #print(blendshapes_data_encoded_eyelids.shape)\n",
    "\n",
    "    blendshapes_data_encoded_expr    = torch.tensor(blendshapes_data_encoded_expr, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_gpose   = torch.tensor(blendshapes_data_encoded_gpose, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_jaw     = torch.tensor(blendshapes_data_encoded_jaw, dtype=torch.float32).to(device)\n",
    "    #blendshapes_data_encoded_eyelids = torch.tensor(blendshapes_data_encoded_eyelids, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_eyelids = None\n",
    "\n",
    "    #vertices_data_encoded = torch.tensor(vertices_data_encoded, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Compute vertices from blendshapes\n",
    "    blendshapes_derived_vertices = get_vertices_from_blendshapes(blendshapes_data_encoded_expr,blendshapes_data_encoded_gpose, blendshapes_data_encoded_jaw, blendshapes_data_encoded_eyelids)\n",
    "    #blendshapes_derived_vertices = vertices_data_encoded\n",
    "    print(\"vertices \", blendshapes_derived_vertices.shape)\n",
    "    \n",
    "    # Fixed camera\n",
    "    cam_original = torch.tensor([5,0,0], dtype=torch.float32).expand(blendshapes_derived_vertices.shape[0], -1).to(device)\n",
    "    print(\"cam \", cam_original.shape)\n",
    "\n",
    "    # Render the frames\n",
    "    renderer_output_blendshapes = renderer.forward(blendshapes_derived_vertices, cam_original)\n",
    "    renderer_output_blendshapes = renderer_output_blendshapes['rendered_img']\n",
    "\n",
    "    #N = renderer_output_blendshapes['rendered_img'].shape[0] # Number of frames\n",
    "    N = renderer_output_blendshapes.shape[0] # Number of frames\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(5, 5),tight_layout=False)\n",
    "\n",
    "    # Create an animation\n",
    "    ani = animation.FuncAnimation(\n",
    "                                    fig, \n",
    "                                    update, \n",
    "                                    frames=N, \n",
    "                                    fargs=(renderer_output_blendshapes, axes),\n",
    "                                    interval=100\n",
    "                                )\n",
    "\n",
    "    # Save the animation as a video file\n",
    "    video_file = f'{output_dir}/{base_name}.mp4'\n",
    "    ani.save(video_file, writer='ffmpeg', fps=25)\n",
    "    print(f\"Video saved as {video_file}\")\n",
    "    \n",
    "    # =============== Add audio to the video ===============\n",
    "    \n",
    "    # Add audio to the video\n",
    "    audio_file = f'{audio_dir}/{base_name}.wav'\n",
    "    output_with_audio = f'{output_dir}/{base_name}_with_audio.mp4'\n",
    "    if os.path.exists(audio_file):\n",
    "        cmd = f'ffmpeg -y -i {video_file} -i {audio_file} -c:v copy -c:a aac -strict experimental {output_with_audio}'\n",
    "        subprocess.run(cmd, shell=True)\n",
    "        print(f\"Video with audio saved as {output_with_audio}\")\n",
    "    else:\n",
    "        print(f\"Audio file {audio_file} not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing encoded files\n",
    "encoded_dir = '/mnt/fasttalk/demo/output' #'/root/Datasets/ARTalk_data/converted_data/npz'\n",
    "audio_dir   = '/mnt/fasttalk/demo/input'  #'/root/Datasets/ARTalk_data/converted_data/wav'\n",
    "output_dir  = 'demo/video'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Directory created: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {output_dir}\")\n",
    "\n",
    "counter = 20\n",
    "\n",
    "file_list = os.listdir(encoded_dir)\n",
    "random.shuffle(file_list)\n",
    "\n",
    "# Iterate over all files in the encoded directory\n",
    "for file_name in file_list:\n",
    "    if counter == 0:\n",
    "        break\n",
    "    if file_name.endswith('.npz'):\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        create_and_save_video(encoded_dir,file_name,renderer,audio_dir,output_dir)\n",
    "    counter -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024d054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasttalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
