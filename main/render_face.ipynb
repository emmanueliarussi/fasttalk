{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# change base folder\n",
    "os.chdir('../')\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from flame.FLAME import FlameHead\n",
    "from renderer.renderer import Renderer\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.io import load_obj, save_ply\n",
    "from pytorch3d.transforms import rotation_6d_to_matrix, matrix_to_rotation_6d, matrix_to_euler_angles, euler_angles_to_matrix\n",
    "import subprocess\n",
    "import tempfile \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n",
    "import glob\n",
    "from models.stage2 import CodeTalker\n",
    "import yaml\n",
    "from models import get_model\n",
    "from base.baseTrainer import load_state_dict\n",
    "from types import SimpleNamespace\n",
    "from transformers import AutoProcessor, Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flame    = FlameHead(shape_params=300,expr_params=50).to(device)\n",
    "renderer = Renderer(render_full_head=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_flatten_yaml(config_path):\n",
    "    \"\"\"\n",
    "    Loads the YAML file and flattens the structure so that\n",
    "    all sub-keys under top-level sections (e.g., DATA, NETWORK, etc.)\n",
    "    appear in a single dictionary without the top-level keys.\n",
    "    \"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        full_config = yaml.safe_load(f)\n",
    "\n",
    "    # Flatten the dict by merging all sub-dicts\n",
    "    flattened_config = {}\n",
    "    for top_level_key, sub_dict in full_config.items():\n",
    "        # sub_dict should itself be a dict of key-value pairs\n",
    "        if isinstance(sub_dict, dict):\n",
    "            # Merge each sub-key into flattened_config\n",
    "            for k, v in sub_dict.items():\n",
    "                flattened_config[k] = v\n",
    "        else:\n",
    "            # In case there's a non-dict top-level key (unlikely but possible)\n",
    "            flattened_config[top_level_key] = sub_dict\n",
    "\n",
    "    return SimpleNamespace(**flattened_config)\n",
    "\n",
    "# 1. Load YAML data into a Python dictionary\n",
    "global cfg\n",
    "cfg = load_and_flatten_yaml(\"config/multi/demo.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(cfg)\n",
    "model = model.to(device)\n",
    "\n",
    "if os.path.isfile(cfg.model_path):\n",
    "    print(\"=> loading checkpoint '{}'\".format(cfg.model_path))\n",
    "    checkpoint = torch.load(cfg.model_path, map_location=lambda storage, loc: storage.cpu())\n",
    "    load_state_dict(model, checkpoint['state_dict'], strict=False)\n",
    "    print(\"=> loaded checkpoint '{}'\".format(cfg.model_path))\n",
    "else:\n",
    "    raise RuntimeError(\"=> no checkpoint flound at '{}'\".format(cfg.model_path))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "checkpoint = torch.load(cfg.model_path, map_location=lambda storage, loc: storage.cpu())\n",
    "\n",
    "audio_input = \"demo/input\"\n",
    "save_folder = \"demo/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to eval and predict blenshapes from audio\n",
    "model.eval()\n",
    "template_file = os.path.join('assets/FLAME2023/', cfg.template_file)\n",
    "with open(template_file, 'rb') as fin:\n",
    "    print(\"Loading template from: \", template_file)\n",
    "    templates = pickle.load(fin, encoding='latin1')\n",
    "    template  = torch.FloatTensor(templates[\"v_template\"].reshape((-1))).to(device='cuda').unsqueeze(0)\n",
    "\n",
    "for wav_file in glob.glob(os.path.join(audio_input,\"*.wav\")):\n",
    "    print('Generating facial animation for {}...'.format(wav_file))\n",
    "    test_name   = os.path.basename(wav_file).split(\".\")[0]\n",
    "    \n",
    "    predicted_blendhsapes_path = os.path.join(save_folder, test_name+'.npz')\n",
    "    speech_array, _ = librosa.load(wav_file, sr=16000)\n",
    "\n",
    "    # Use Wav2Vec audio features\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "    audio_feature = np.squeeze(processor(speech_array, sampling_rate=16000).input_values)\n",
    "    audio_feature = np.reshape(audio_feature, (-1, audio_feature.shape[0]))\n",
    "    audio_feature = torch.FloatTensor(audio_feature).to(device='cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vertice_out, expr_out = model.predict(audio_feature,template)\n",
    "        \n",
    "        expr_out, jaw_out, neck_out = torch.split(expr_out, [50, 3, 3], dim=-1)\n",
    "        np.savez(predicted_blendhsapes_path, expr=expr_out.detach().cpu().numpy(), jaw=jaw_out.detach().cpu().numpy(), neck=neck_out.detach().cpu().numpy())\n",
    "        print(f'Save facial animation in {predicted_blendhsapes_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertices_from_blendshapes(expr, jaw, neck=None):\n",
    "    # Load the encoded file\n",
    "    expr_tensor =  expr.to(device)\n",
    "    jaw_tensor  =  jaw.to(device) #torch.zeros(expr_tensor.shape[0],3).to(device)\n",
    "\n",
    "    target_shape_tensor = torch.zeros(expr_tensor.shape[0], 300).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    I = matrix_to_euler_angles(torch.cat([torch.eye(3)[None]], dim=0),\"XYZ\").to(device)\n",
    "\n",
    "    eye_r    = I.clone().to(device).squeeze()\n",
    "    eye_l    = I.clone().to(device).squeeze()\n",
    "    eyes     = torch.cat([eye_r,eye_l],dim=0).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    translation = torch.zeros(expr_tensor.shape[0], 3).to(device)\n",
    "\n",
    "    if neck==None:\n",
    "        neck = I.clone().expand(expr_tensor.shape[0], -1).to(device)\n",
    "    \n",
    "    rotation = I.clone().expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    # Compute Flame\n",
    "    flame_output_only_shape   = flame.forward(target_shape_tensor, expr_tensor, rotation, neck, jaw_tensor, eyes, translation, return_landmarks=False)\n",
    "\n",
    "    return flame_output_only_shape.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(frame_inx, renderer_output_blendshapes, axes):\n",
    "    # Select the frames to plot\n",
    "    frame = renderer_output_blendshapes['rendered_img'][frame_inx].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Update the second subplot\n",
    "    axes.clear()\n",
    "    axes.imshow((frame * 255).astype(np.uint8))\n",
    "    axes.axis('off')\n",
    "    axes.set_title(f'Frame Stage 1 (Blendshape) {frame_inx + 1}')\n",
    "\n",
    "# Function to create and save the video\n",
    "def create_and_save_video(encoded_dir,file_name, renderer,audio_dir,output_dir):\n",
    "    base_name = os.path.basename(file_name).replace('.npz', '')\n",
    "    print(base_name)\n",
    "    \n",
    "    blendshapes_data_encoded_expr = np.load(f'{encoded_dir}/{base_name}.npz')['expr'].reshape(-1, 50)\n",
    "    blendshapes_data_encoded_jaw  = np.load(f'{encoded_dir}/{base_name}.npz')['jaw'].reshape(-1, 3)\n",
    "    blendshapes_data_encoded_neck  = np.load(f'{encoded_dir}/{base_name}.npz')['neck'].reshape(-1, 3)\n",
    "\n",
    "    blendshapes_data_encoded_expr = torch.tensor(blendshapes_data_encoded_expr, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_jaw  = torch.tensor(blendshapes_data_encoded_jaw, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_neck = torch.tensor(blendshapes_data_encoded_neck, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Compute vertices from blendshapes\n",
    "    blendshapes_derived_vertices = get_vertices_from_blendshapes(blendshapes_data_encoded_expr,blendshapes_data_encoded_jaw, blendshapes_data_encoded_neck)\n",
    "    print(blendshapes_derived_vertices.shape)\n",
    "    \n",
    "    # Fixed camera\n",
    "    cam_original = torch.tensor([10,0,0], dtype=torch.float32).expand(blendshapes_derived_vertices.shape[0], -1).to(device)\n",
    "    print(cam_original.shape)\n",
    "\n",
    "    # Render the frames\n",
    "    renderer_output_blendshapes  = renderer.forward(blendshapes_derived_vertices, cam_original)\n",
    "\n",
    "    N = renderer_output_blendshapes['rendered_img'].shape[0] # Number of frames\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "    # Create an animation\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, \n",
    "        update, \n",
    "        frames=N, \n",
    "        fargs=(renderer_output_blendshapes, axes),\n",
    "        interval=100\n",
    "    )\n",
    "\n",
    "    # Save the animation as a video file\n",
    "    video_file = f'{output_dir}/{base_name}.mp4'\n",
    "    ani.save(video_file, writer='ffmpeg', fps=25)\n",
    "    print(f\"Video saved as {video_file}\")\n",
    "    \n",
    "    # =============== Add audio to the video ===============\n",
    "    \n",
    "    # Add audio to the video\n",
    "    audio_file = f'{audio_dir}/{base_name}.wav'\n",
    "    output_with_audio = f'{output_dir}/{base_name}_with_audio.mp4'\n",
    "    if os.path.exists(audio_file):\n",
    "        cmd = f'ffmpeg -y -i {video_file} -i {audio_file} -c:v copy -c:a aac -strict experimental {output_with_audio}'\n",
    "        subprocess.run(cmd, shell=True)\n",
    "        print(f\"Video with audio saved as {output_with_audio}\")\n",
    "    else:\n",
    "        print(f\"Audio file {audio_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing encoded files\n",
    "encoded_dir = 'demo/output'\n",
    "audio_dir   = 'demo/input'\n",
    "output_dir  = 'demo/output'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Directory created: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {output_dir}\")\n",
    "\n",
    "counter =  20\n",
    "# Iterate over all files in the encoded directory\n",
    "for file_name in os.listdir(encoded_dir):\n",
    "    if counter == 0:\n",
    "        break\n",
    "    if file_name.endswith('.npz'):\n",
    "        create_and_save_video(encoded_dir,file_name,renderer,audio_dir,output_dir)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multitalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
