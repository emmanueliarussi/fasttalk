{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# change base folder\n",
    "os.chdir('./fasttalk/')\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from flame.FLAME import FlameHead\n",
    "from renderer.renderer import Renderer\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.io import load_obj,save_ply\n",
    "from pytorch3d.transforms import rotation_6d_to_matrix, matrix_to_rotation_6d, matrix_to_euler_angles, euler_angles_to_matrix\n",
    "import subprocess\n",
    "import tempfile \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flame    = FlameHead(shape_params=300,expr_params=50).to(device)\n",
    "renderer = Renderer(render_full_head=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertices_from_blendshapes(expr, jaw, neck=None):\n",
    "    # Load the encoded file\n",
    "    expr_tensor =  expr.to(device)\n",
    "    jaw_tensor  =  jaw.to(device) #torch.zeros(expr_tensor.shape[0],3).to(device)\n",
    "\n",
    "    target_shape_tensor = torch.zeros(expr_tensor.shape[0], 300).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    I = matrix_to_euler_angles(torch.cat([torch.eye(3)[None]], dim=0),\"XYZ\").to(device)\n",
    "\n",
    "    eye_r    = I.clone().to(device).squeeze()\n",
    "    eye_l    = I.clone().to(device).squeeze()\n",
    "    eyes     = torch.cat([eye_r,eye_l],dim=0).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    translation = torch.zeros(expr_tensor.shape[0], 3).to(device)\n",
    "\n",
    "    if neck==None:\n",
    "        neck = I.clone().expand(expr_tensor.shape[0], -1).to(device)\n",
    "    \n",
    "    rotation = I.clone().expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    # Compute Flame\n",
    "    flame_output_only_shape   = flame.forward(target_shape_tensor, expr_tensor, rotation, neck, jaw_tensor, eyes, translation, return_landmarks=False)\n",
    "\n",
    "    return flame_output_only_shape.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(frame_inx, renderer_output_blendshapes, axes):\n",
    "    # Select the frames to plot\n",
    "    frame = renderer_output_blendshapes['rendered_img'][frame_inx].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Update the second subplot\n",
    "    axes.clear()\n",
    "    axes.imshow((frame * 255).astype(np.uint8))\n",
    "    axes.axis('off')\n",
    "    axes.set_title(f'Frame Stage 1 (Blendshape) {frame_inx + 1}')\n",
    "\n",
    "# Function to create and save the video\n",
    "def create_and_save_video(encoded_dir,file_name, renderer,audio_dir,output_dir):\n",
    "    base_name = os.path.basename(file_name).replace('.npz', '')\n",
    "    print(base_name)\n",
    "    \n",
    "    blendshapes_data_encoded_expr = np.load(f'{encoded_dir}/{base_name}.npz')['expr'].reshape(-1, 50)\n",
    "    blendshapes_data_encoded_jaw  = np.load(f'{encoded_dir}/{base_name}.npz')['jaw'].reshape(-1, 3)\n",
    "    blendshapes_data_encoded_neck  = np.load(f'{encoded_dir}/{base_name}.npz')['neck'].reshape(-1, 3)\n",
    "\n",
    "    blendshapes_data_encoded_expr = torch.tensor(blendshapes_data_encoded_expr, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_jaw  = torch.tensor(blendshapes_data_encoded_jaw, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_neck = torch.tensor(blendshapes_data_encoded_neck, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Compute vertices from blendshapes\n",
    "    blendshapes_derived_vertices = get_vertices_from_blendshapes(blendshapes_data_encoded_expr,blendshapes_data_encoded_jaw, blendshapes_data_encoded_neck)\n",
    "    print(blendshapes_derived_vertices.shape)\n",
    "    \n",
    "    # Fixed camera\n",
    "    cam_original = torch.tensor([10,0,0], dtype=torch.float32).expand(blendshapes_derived_vertices.shape[0], -1).to(device)\n",
    "    print(cam_original.shape)\n",
    "\n",
    "    # Render the frames\n",
    "    renderer_output_blendshapes  = renderer.forward(blendshapes_derived_vertices, cam_original)\n",
    "\n",
    "    N = renderer_output_blendshapes['rendered_img'].shape[0] # Number of frames\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "    # Create an animation\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, \n",
    "        update, \n",
    "        frames=N, \n",
    "        fargs=(renderer_output_blendshapes, axes),\n",
    "        interval=100\n",
    "    )\n",
    "\n",
    "    # Save the animation as a video file\n",
    "    video_file = f'{output_dir}/{base_name}.mp4'\n",
    "    ani.save(video_file, writer='ffmpeg', fps=25)\n",
    "    print(f\"Video saved as {video_file}\")\n",
    "    \n",
    "    # =============== Add audio to the video ===============\n",
    "    \n",
    "    # Add audio to the video\n",
    "    audio_file = f'{audio_dir}/{base_name}.wav'\n",
    "    output_with_audio = f'{output_dir}/{base_name}_with_audio.mp4'\n",
    "    if os.path.exists(audio_file):\n",
    "        cmd = f'ffmpeg -y -i {video_file} -i {audio_file} -c:v copy -c:a aac -strict experimental {output_with_audio}'\n",
    "        subprocess.run(cmd, shell=True)\n",
    "        print(f\"Video with audio saved as {output_with_audio}\")\n",
    "    else:\n",
    "        print(f\"Audio file {audio_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing encoded files\n",
    "encoded_dir = 'demo/output'\n",
    "audio_dir   = 'demo/input'\n",
    "output_dir  = 'demo/output'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Directory created: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {output_dir}\")\n",
    "\n",
    "counter =  20\n",
    "# Iterate over all files in the encoded directory\n",
    "for file_name in os.listdir(encoded_dir):\n",
    "    if counter == 0:\n",
    "        break\n",
    "    if file_name.endswith('.npz'):\n",
    "        create_and_save_video(encoded_dir,file_name,renderer,audio_dir,output_dir)\n",
    "    counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multitalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
